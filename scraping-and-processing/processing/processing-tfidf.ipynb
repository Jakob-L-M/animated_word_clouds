{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, date, timedelta\n",
    "import glob\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from math import log\n",
    "import heapq\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = r'../scraping/data/storage/'\n",
    "keywords_location = r'./keywords.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_date(timestamp):\n",
    "    return date.fromtimestamp(timestamp)\n",
    "\n",
    "def format_text(text):\n",
    "    # remove Uni-Code\n",
    "    t = text.encode('latin-1', 'ignore').decode('latin-1')\n",
    "    \n",
    "    # Hashtags entfernen\n",
    "    t = re.sub(r'#', '', t)\n",
    "    \n",
    "    # Links entfernen\n",
    "    t = re.sub(r'http\\S+', '', t)\n",
    "    \n",
    "    # remove Steuersymbole/andere Zeichen\n",
    "    t = re.sub(r'[\\n\\t\\ \\\"\\':+?!]+', ' ', t)\n",
    "    \n",
    "    # remove \\xad\n",
    "    t = re.sub(r'\\xad', '', t)\n",
    "    \n",
    "    return t\n",
    "\n",
    "def format_words(word):\n",
    "    return re.sub('[\\[\\-.,:?&()!@#|$0-9 ]', '', word)\n",
    "\n",
    "# filling stop_words\n",
    "# Using stop words form https://countwordsfree.com/stopwords/german\n",
    "stop_words = []\n",
    "# TODO verbessern\n",
    "with open(r'stop_words_german.json', 'r', encoding='utf-8') as words:\n",
    "    stop_words = json.load(words)\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "# reading keywords\n",
    "keywords = []\n",
    "with open(keywords_location, mode='r', encoding='utf-8') as file:\n",
    "    keywords = json.load(file)\n",
    "\n",
    "def frame_to_dic(dataframe):\n",
    "    dic = {}\n",
    "    for i in dataframe['text'].to_numpy():\n",
    "        for j in nlp(i):\n",
    "            # todo nummern aussortieren\n",
    "            if len(j.lemma_) > 3 and j.lemma_ not in stop_words and '@' not in j.lemma_ and '-' not in j.lemma_:\n",
    "                s = (j.lemma_).upper()\n",
    "            else:\n",
    "                continue\n",
    "            if (s in dic):\n",
    "                dic[s] += 1\n",
    "            else:\n",
    "                dic[s] = 1\n",
    "    return dic\n",
    "\n",
    "def get_dic_around(date, dic_frame, day_range):\n",
    "    res = {}\n",
    "    for i in dic_frame[(dic_frame['date'] >= (date - timedelta(days=day_range))) & (dic_frame['date'] <= (date + timedelta(days=day_range)))]['dic']:\n",
    "        if type(i) == str:\n",
    "            i = json.loads(i.replace(\"'\", \"\\\"\"))\n",
    "        res = {**res, **i}\n",
    "    return res\n",
    "\n",
    "def get_top_k(dictionary, k):\n",
    "    if type(dictionary) == str:\n",
    "        dictionary = json.loads(dictionary.replace(\"'\", \"\\\"\"))\n",
    "    n = sum(sorted(dictionary.values(), reverse=True)[:k])\n",
    "    sort_dic = sorted(dictionary, key=dictionary.get, reverse=True)\n",
    "    return {i: dictionary[i]/n for i in list(sort_dic)[:k]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(data, file):\n",
    "    with open(file, mode='w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.date_range(min(dic_frame['date']), max(dic_frame['date']))\n",
    "all_words = set([])\n",
    "res = []\n",
    "word_index = {}\n",
    "weight_dic = [0] * len(days)\n",
    "counter = 0\n",
    "\n",
    "for ind,day in tqdm(enumerate(days)):\n",
    "    word_dic = get_top_k(get_dic_around(date=day, dic_frame=dic_frame, day_range=7), 10)\n",
    "    words = list(word_dic.keys())\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            res[word_index[word]][\"days\"].append(ind)\n",
    "        else:\n",
    "            word_index[word] = counter\n",
    "            res.append({\"word\": word, \"days\":[ind]})\n",
    "            counter += 1\n",
    "        all_words.add(word)\n",
    "    day_dic = {'words': words, 'weights': list(word_dic.values())}\n",
    "    # write_json(day_dic, '../../data/corona/days/' + str(ind) + '.json')\n",
    "    weight_dic[ind] = day_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_be_removed = []\n",
    "\n",
    "r = res.copy()\n",
    "\n",
    "for ind, i in enumerate(r):\n",
    "    day_set = set(i['days'])\n",
    "    if len(day_set) < 5:\n",
    "        index_to_be_removed.append(ind)\n",
    "        continue\n",
    "    new_days = []\n",
    "    start_day = 0\n",
    "    for j in sorted(i['days']):\n",
    "        if j-1 not in day_set:\n",
    "            start_day = j\n",
    "        if j+1 in day_set:\n",
    "            continue\n",
    "        else:\n",
    "            new_days.append([start_day, j + 1])\n",
    "    i['days'] = new_days\n",
    "\n",
    "counter = 0\n",
    "for ind in index_to_be_removed:\n",
    "    res.pop(ind - counter)\n",
    "    counter += 1\n",
    "\n",
    "write_json(res, r'../../corona_data/main.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res:\n",
    "    w = []\n",
    "    for day in i['days']:\n",
    "        ind = weight_dic[day]['words'].index(i['word'])\n",
    "        w.append(weight_dic[day]['weights'][ind])\n",
    "    write_json(w, '../../data/corona/words/' + i['word'] + '.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['date', 'user', 'shortcode', 'platform', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(storage + '*.json')\n",
    "\n",
    "for j in tqdm(files):\n",
    "    # Öffnen einer JSON Datei \n",
    "    f = open(j, mode=\"r\", encoding=\"utf-8\") \n",
    "\n",
    "    # JSON als dictionary \n",
    "    data = json.load(f)\n",
    "\n",
    "    for i in data:\n",
    "        \n",
    "        text = i['text'].lower()\n",
    "        \n",
    "        # checking if any of the keywords is in the given sequence\n",
    "        if any(word in text for word in keywords):\n",
    "            \n",
    "            date = timestamp_to_date(i['date'])\n",
    "            user = i['user']\n",
    "            shortcode = i['shortcode']\n",
    "            platform = i['platform']\n",
    "            text = format_text(i['text'])\n",
    "            df = df.append({'date': date, 'user': user,\n",
    "                            'shortcode': shortcode, 'platform': platform, \n",
    "                            'text': text}, ignore_index=True)\n",
    "\n",
    "    # Closing file \n",
    "    f.close()\n",
    "df.to_csv(\"tweet_frame.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tweet_frame.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['date'] >= date(2020, 1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes a dataframe, returns an array with the same length as the amount of days\n",
    "The index of the array can be mapped to sorted(df['date'].unique())\n",
    "\n",
    "Each element in the array (string) consists of all words assosiated with that day\n",
    "\n",
    "Will be used for tfidf calculation\n",
    "\"\"\"\n",
    "date_range = pd.date_range(min(df['date'].unique()), max(df['date'].unique()))\n",
    "\n",
    "def stem_date_tweets(df):\n",
    "    res = []\n",
    "    for i in tqdm(date_range):\n",
    "        sentence = []\n",
    "        for tweet in df[df['date'] == i]['text']:\n",
    "            for word in nlp(tweet):\n",
    "                if len(word.lemma_) < 2:\n",
    "                    continue\n",
    "                if word.lemma_ in stop_words or word.text_ in stop_words:\n",
    "                    continue\n",
    "                if any(number in word.lemma_ for number in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']):\n",
    "                    continue\n",
    "                sentence.append(word.lemma_)\n",
    "        res.append(\" \".join(sentence))\n",
    "    return res\n",
    "\n",
    "stemed_tweets = stem_date_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X is a #days x #unique words big matrix\n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(stemed_tweets)\n",
    "X_words = vectorizer.get_feature_names()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hier werden die geglätteten dics gebaut\n",
    "\"\"\"\n",
    "date_arr = []\n",
    "for day in tqdm(range(0, len(date_range))):\n",
    "    arr = list(np.sum(list(np.array(X.todense()[max(0,day - 3):min(num_of_words, day + 4)])), axis = 0))\n",
    "    top_k_ind = list(map(arr.index, heapq.nlargest(15, arr)))\n",
    "    top_k_w = heapq.nlargest(15, arr)\n",
    "    temp = {}\n",
    "    for i, word in enumerate(top_k_ind):\n",
    "        temp[X_words[word]] = top_k_w[i]\n",
    "    date_arr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_arr[350:353]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range[350]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
