{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, date, timedelta\n",
    "import glob\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from math import log\n",
    "import heapq\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = r'../scraping/data/storage/'\n",
    "keywords_location = r'./keywords.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_date(timestamp):\n",
    "    return date.fromtimestamp(timestamp)\n",
    "\n",
    "def format_text(text):\n",
    "    # remove Uni-Code\n",
    "    t = text.encode('latin-1', 'ignore').decode('latin-1')\n",
    "    \n",
    "    # Hashtags entfernen\n",
    "    t = re.sub(r'#', '', t)\n",
    "    \n",
    "    # Links entfernen\n",
    "    t = re.sub(r'http\\S+', '', t)\n",
    "    \n",
    "    # remove Steuersymbole/andere Zeichen\n",
    "    t = re.sub(r'[\\n\\t\\ \\\"\\':+?!]+', ' ', t)\n",
    "    \n",
    "    # remove \\xad\n",
    "    t = re.sub(r'\\xad', '', t)\n",
    "    \n",
    "    return t\n",
    "\n",
    "def format_words(word):\n",
    "    return re.sub('[\\[\\-.,:?&()!@#|$0-9 ]', '', word)\n",
    "\n",
    "# filling stop_words\n",
    "# Using stop words form https://countwordsfree.com/stopwords/german\n",
    "stop_words = []\n",
    "# TODO verbessern\n",
    "with open(r'stop_words_german.json', 'r', encoding='utf-8') as words:\n",
    "    stop_words = json.load(words)\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "# reading keywords\n",
    "keywords = []\n",
    "with open(keywords_location, mode='r', encoding='utf-8') as file:\n",
    "    keywords = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.date_range(min(dic_frame['date']), max(dic_frame['date']))\n",
    "all_words = set([])\n",
    "res = []\n",
    "word_index = {}\n",
    "weight_dic = [0] * len(days)\n",
    "counter = 0\n",
    "\n",
    "for ind,day in tqdm(enumerate(days)):\n",
    "    word_dic = get_top_k(get_dic_around(date=day, dic_frame=dic_frame, day_range=7), 10)\n",
    "    words = list(word_dic.keys())\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            res[word_index[word]][\"days\"].append(ind)\n",
    "        else:\n",
    "            word_index[word] = counter\n",
    "            res.append({\"word\": word, \"days\":[ind]})\n",
    "            counter += 1\n",
    "        all_words.add(word)\n",
    "    day_dic = {'words': words, 'weights': list(word_dic.values())}\n",
    "    # write_json(day_dic, '../../data/corona/days/' + str(ind) + '.json')\n",
    "    weight_dic[ind] = day_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_be_removed = []\n",
    "\n",
    "r = res.copy()\n",
    "\n",
    "for ind, i in enumerate(r):\n",
    "    day_set = set(i['days'])\n",
    "    if len(day_set) < 5:\n",
    "        index_to_be_removed.append(ind)\n",
    "        continue\n",
    "    new_days = []\n",
    "    start_day = 0\n",
    "    for j in sorted(i['days']):\n",
    "        if j-1 not in day_set:\n",
    "            start_day = j\n",
    "        if j+1 in day_set:\n",
    "            continue\n",
    "        else:\n",
    "            new_days.append([start_day, j + 1])\n",
    "    i['days'] = new_days\n",
    "\n",
    "counter = 0\n",
    "for ind in index_to_be_removed:\n",
    "    res.pop(ind - counter)\n",
    "    counter += 1\n",
    "\n",
    "write_json(res, r'../../corona_data/main.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in res:\n",
    "    w = []\n",
    "    for day in i['days']:\n",
    "        ind = weight_dic[day]['words'].index(i['word'])\n",
    "        w.append(weight_dic[day]['weights'][ind])\n",
    "    write_json(w, '../../data/corona/words/' + i['word'] + '.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['date', 'user', 'shortcode', 'platform', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 16/16 [08:44<00:00, 32.75s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "to_csv() got an unexpected keyword argument 'index_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9b03c05bc009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Closing file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tweet_frame.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: to_csv() got an unexpected keyword argument 'index_col'"
     ]
    }
   ],
   "source": [
    "files = glob.glob(storage + '*.json')\n",
    "\n",
    "for j in tqdm(files):\n",
    "    # Öffnen einer JSON Datei \n",
    "    f = open(j, mode=\"r\", encoding=\"utf-8\") \n",
    "\n",
    "    # JSON als dictionary \n",
    "    data = json.load(f)\n",
    "\n",
    "    for i in data:\n",
    "        \n",
    "        text = i['text'].lower()\n",
    "        \n",
    "        # checking if any of the keywords is in the given sequence\n",
    "        if any(word in text for word in keywords):\n",
    "            \n",
    "            date = timestamp_to_date(i['date'])\n",
    "            user = i['user']\n",
    "            shortcode = i['shortcode']\n",
    "            platform = i['platform']\n",
    "            text = format_text(i['text'])\n",
    "            df = df.append({'date': date, 'user': user,\n",
    "                            'shortcode': shortcode, 'platform': platform, \n",
    "                            'text': text}, ignore_index=True)\n",
    "\n",
    "    # Closing file \n",
    "    f.close()\n",
    "df.to_csv(\"tweet_frame.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If already build once, data can be read in here\n",
    "df = pd.read_csv(\"tweet_frame.csv\", index_col=False)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['date'] >= pd.to_datetime(\"2020-01-05\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes a dataframe, returns an array with the same length as the amount of days\n",
    "The index of the array can be mapped to sorted(df['date'].unique())\n",
    "\n",
    "Each element in the array (string) consists of all words assosiated with that day\n",
    "\n",
    "Will be used for tfidf calculation\n",
    "\"\"\"\n",
    "date_range = pd.date_range(min(df['date'].unique()), max(df['date'].unique()))\n",
    "\n",
    "def stem_date_tweets(df):\n",
    "    res = []\n",
    "    for i in tqdm(date_range):\n",
    "        sentence = []\n",
    "        for tweet in df[df['date'] == i]['text']:\n",
    "            for word in nlp(tweet):\n",
    "                if len(word.lemma_) < 2:\n",
    "                    continue\n",
    "                if word.lemma_ in stop_words or word.text_ in stop_words:\n",
    "                    continue\n",
    "                if any(number in word.lemma_ for number in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']):\n",
    "                    continue\n",
    "                sentence.append(word.lemma_)\n",
    "        res.append(\" \".join(sentence))\n",
    "    return res\n",
    "\n",
    "stemed_tweets = stem_date_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X is a #days x #unique words big matrix\n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(stemed_tweets)\n",
    "X_words = vectorizer.get_feature_names()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Smoothing of tf-idf, also ignoring word that appear on less than 100th of days\n",
    "\n",
    "todo:   -no idea if its working properly\n",
    "        -smoothing\n",
    "\"\"\"\n",
    "date_arr = []\n",
    "for day in tqdm(range(0, len(date_range))):\n",
    "    arr = list(np.sum(list(np.array(X.todense()[max(0,day - 3):min(num_of_words, day + 4)])), axis = 0))\n",
    "    top_k_ind = list(map(arr.index, heapq.nlargest(15, arr)))\n",
    "    top_k_w = heapq.nlargest(15, arr)\n",
    "    temp = {}\n",
    "    for i, word in enumerate(top_k_ind):\n",
    "        temp[X_words[word]] = top_k_w[i]\n",
    "    date_arr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exporter\n",
    "#todo: call of exporter\n",
    "    # tests for exporter\n",
    "    #finish exporter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
