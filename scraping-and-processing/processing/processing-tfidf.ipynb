{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, date, timedelta\n",
    "import glob\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from math import log\n",
    "import heapq\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nlp = spacy.load('de_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = r'../scraping/data/storage/'\n",
    "keywords_location = r'./keywords.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_to_date(timestamp):\n",
    "    return date.fromtimestamp(timestamp)\n",
    "\n",
    "def format_text(text):\n",
    "    # remove Uni-Code\n",
    "    t = text.encode('latin-1', 'ignore').decode('latin-1')\n",
    "    \n",
    "    # Hashtags entfernen\n",
    "    t = re.sub(r'#', '', t)\n",
    "    \n",
    "    # Links entfernen\n",
    "    t = re.sub(r'http\\S+', '', t)\n",
    "    \n",
    "    # remove Steuersymbole/andere Zeichen\n",
    "    t = re.sub(r'[\\n\\t\\ \\\"\\':+?!]+', ' ', t)\n",
    "    \n",
    "    # remove \\xad\n",
    "    t = re.sub(r'\\xad', '', t)\n",
    "    \n",
    "    return t\n",
    "\n",
    "def format_words(word):\n",
    "    return re.sub('[\\[\\-.,:?&()!@#|$0-9 ]', '', word)\n",
    "\n",
    "# filling stop_words\n",
    "# Using stop words form https://countwordsfree.com/stopwords/german\n",
    "stop_words = []\n",
    "# TODO verbessern\n",
    "with open(r'stop_words_german.json', 'r', encoding='utf-8') as words:\n",
    "    stop_words = json.load(words)\n",
    "stop_words = set(stop_words)\n",
    "\n",
    "# reading keywords\n",
    "keywords = []\n",
    "with open(keywords_location, mode='r', encoding='utf-8') as file:\n",
    "    keywords = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initilize empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['date', 'user', 'shortcode', 'platform', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(storage + '*.json')\n",
    "\n",
    "for j in tqdm(files):\n",
    "    # Öffnen einer JSON Datei \n",
    "    f = open(j, mode=\"r\", encoding=\"utf-8\") \n",
    "\n",
    "    # JSON als dictionary \n",
    "    data = json.load(f)\n",
    "\n",
    "    for i in data:\n",
    "        \n",
    "        text = i['text'].lower()\n",
    "        \n",
    "        # checking if any of the keywords is in the given sequence\n",
    "        if any(word in text for word in keywords):\n",
    "            \n",
    "            date = timestamp_to_date(i['date'])\n",
    "            user = i['user']\n",
    "            shortcode = i['shortcode']\n",
    "            platform = i['platform']\n",
    "            text = format_text(i['text'])\n",
    "            df = df.append({'date': date, 'user': user,\n",
    "                            'shortcode': shortcode, 'platform': platform, \n",
    "                            'text': text}, ignore_index=True)\n",
    "\n",
    "    # Closing file \n",
    "    f.close()\n",
    "df.to_csv(\"tweet_frame.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If already build once, data can be read in here\n",
    "df = pd.read_csv(\"tweet_frame.csv\", index_col=False)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[df['date'] >= pd.to_datetime(\"2020-01-07\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Takes a dataframe, returns an array with the same length as the amount of days\n",
    "The index of the array can be mapped to sorted(df['date'].unique())\n",
    "\n",
    "Each element in the array (string) consists of all words assosiated with that day\n",
    "\n",
    "Will be used for tfidf calculation\n",
    "\"\"\"\n",
    "date_range = pd.date_range(min(df['date'].unique()), max(df['date'].unique()))\n",
    "\n",
    "def stem_date_tweets(df):\n",
    "    res = []\n",
    "    for i in tqdm(date_range):\n",
    "        sentence = []\n",
    "        for tweet in df[df['date'] == i]['text']:\n",
    "            for word in nlp(tweet):\n",
    "                if len(word.text) < 3:\n",
    "                    continue\n",
    "                if word.lemma_ in stop_words or word.text in stop_words:\n",
    "                    continue\n",
    "                if any(number in word.lemma_ for number in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']):\n",
    "                    continue\n",
    "                sentence.append(word.lemma_)\n",
    "        res.append(\" \".join(sentence))\n",
    "    return res\n",
    "\n",
    "stemed_tweets = stem_date_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X is a #days x #unique words big matrix\n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(stemed_tweets)\n",
    "X_words = vectorizer.get_feature_names()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Smoothing of tf-idf, also ignoring word that appear on less than 100th of days\n",
    "\n",
    "todo:   -no idea if its working properly\n",
    "        -smoothing\n",
    "\"\"\"\n",
    "date_arr = []\n",
    "day_smoothing = 3\n",
    "k = 25\n",
    "for day in tqdm(range(0, len(date_range))):\n",
    "    arr = list(np.sum(list(np.array(X.todense()[max(0,day - day_smoothing):min(len(date_range), day + day_smoothing)])), axis = 0))\n",
    "    top_k_ind = list(map(arr.index, heapq.nlargest(k, arr)))\n",
    "    top_k_w = heapq.nlargest(k, arr)\n",
    "    top_k_w = list(np.array(top_k_w)/sum(top_k_w))\n",
    "    temp = {'day': day, 'words': [], 'weights': []}\n",
    "    for i, word in enumerate(top_k_ind):\n",
    "        temp['words'].append(X_words[word])\n",
    "        temp['weights'].append(top_k_w[i])\n",
    "    date_arr.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import exporter\n",
    "import importlib\n",
    "importlib.reload(exporter)\n",
    "\n",
    "exporter.export(date_arr, str(min(date_range))[:10], len(date_range), \"corona\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
