{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, date, timezone, timedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = r'./scraping/twitter/config.json'\n",
    "temp_storage = r'./scraping/data/temp_storage/'\n",
    "storage_path = r'./scraping/data/storage/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_storage(storage, categories):\n",
    "    # check if a main file exsists\n",
    "    if not os.path.isfile(storage + 'main.csv'):\n",
    "        main = pd.DataFrame(columns=['id', 'date', 'user', 'text'])\n",
    "        main.to_csv(storage + 'main.csv', index=False)\n",
    "        with open(storage + 'README.txt', 'x') as f:\n",
    "            # todo: add correct link\n",
    "            f.write('For instructions visit github.com/Jakob-L-M/')\n",
    "    \n",
    "    df = pd.DataFrame(columns=['id', 'date', 'user', 'text'])\n",
    "    for category in categories:\n",
    "        os.mkdir(storage+category)\n",
    "        df.to_csv(storage + category + '/data.csv', index=False)\n",
    "        with open(storage + category + '/keywords.json', 'x', encoding='utf-8') as f:\n",
    "            json.dump(['Write','your','keywords','here'], f)\n",
    "        f.close()\n",
    "        with open(storage + category + '/stopwords.json', 'x', encoding='utf-8') as f:\n",
    "            json.dump(['Write','your','stopwords','here'], f)\n",
    "        f.close()\n",
    "        with open(storage + category + '/config.json', 'x') as f:\n",
    "            json.dump({'start_date': '2000-01-01', 'end_date': '', 'day_smoothing': 3, 'k': 25}, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_main(storage, temp_storage):\n",
    "    temp_files = glob(temp_storage + \"*.txt\")\n",
    "    main = pd.read_csv(storage+'main.csv')\n",
    "    for j in tqdm(temp_files):\n",
    "\n",
    "        f = open(j, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "        user_tweets = []\n",
    "\n",
    "        for i in f.readlines():\n",
    "\n",
    "            temp = json.loads(i[:-1])\n",
    "\n",
    "            if temp['retweetedTweet'] is None:\n",
    "\n",
    "                s = temp['date']\n",
    "                # String to datetime.date\n",
    "                dt = datetime(int(s[:4]), int(s[5:7]), int(s[8:10]), int(s[11:13]), int(s[14:16]), int(s[17:19]))\n",
    "\n",
    "                dic = {}\n",
    "\n",
    "                dic['date'] = pd.to_datetime(dt)\n",
    "                dic['text'] = temp['content']\n",
    "                dic['id'] = temp['id']\n",
    "                dic['user'] = temp['user']['username']\n",
    "\n",
    "                user_tweets.append(dic)\n",
    "        # Closing file \n",
    "        f.close()\n",
    "\n",
    "        main = main.append(user_tweets)\n",
    "\n",
    "    main = main.drop_duplicates()  \n",
    "\n",
    "    main.to_csv(storage + 'main.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_category(storage, category):\n",
    "    category_data = pd.read_csv(storage+category+'/data.csv')\n",
    "    main_data = pd.read_csv(storage+'main.csv')\n",
    "    with open(storage+category+'/keywords.json', 'r', encoding='utf-8') as f:\n",
    "        keywords = json.load(f)\n",
    "        use_filter = True\n",
    "        if keywords[0] != \"\":\n",
    "            keyword_filter = lambda x: any(word in x['text'] for word in keywords)\n",
    "        else:\n",
    "            use_filter = False\n",
    "    f.close()\n",
    "    with open(storage+category+'/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    if config['start_date'] != '':\n",
    "        main_data = main_data[main_data['date'] > config['start_date']]\n",
    "        \n",
    "    if config['end_date'] != '':\n",
    "        main_data = main_data[main_data['date'] < config['end_date']]\n",
    "    # if there already exsists data, we will slice the main file first to only check for new entries\n",
    "    if len(category_data['date']) != 0:\n",
    "        main_data = main_data[main_data['date'] > max(category_data['date'])]\n",
    "    \n",
    "    if use_filter:\n",
    "        main_data = main_data[main_data.apply(keyword_filter, axis=1)]\n",
    "    \n",
    "    # append new data\n",
    "    category_data = category_data.append(main_data, ignore_index=True)\n",
    "    \n",
    "    # Just to be 100% safe\n",
    "    category_data = category_data.drop_duplicates()\n",
    "    \n",
    "    category_data.to_csv(storage+category+'/data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
